{"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"],"metadata":{"id":"wxZDXLDCXkk_","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcd007ca-3c00-4a89-bcc6-b54e2eed6e86","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Train Legal Assertion"],"metadata":{"id":"KLqW6FOnEvov","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"31b30c97-6e86-41d6-81c6-5c314a6bbbab","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from johnsnowlabs import * "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JOzbTlCRNvVd","outputId":"aa4536cc-fd00-446b-91d1-2615a159b722","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f88df453-816f-41bf-9113-4a4c8db91218","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Data Prep"],"metadata":{"id":"JYBQyxEd0uR0","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f2409c7-c4c5-40e6-82e1-31c2e5d1dae7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings_JSL/Legal/data/assertion_fin.csv\ndbutils.fs.cp(\"file:/databricks/driver/assertion_fin.csv\", \"dbfs:/Finance\") "],"metadata":{"id":"AVBmGFcQ03La","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40d60008-2e51-434c-873a-42af82248668","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\ntraining_df = pd.read_csv('/dbfs/Finance/assertion_fin.csv')"],"metadata":{"id":"8iJF_HCw1Lgh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"492cd4c7-1803-4804-9af5-bce963af3bca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["training_data = spark.createDataFrame(training_df)\ntraining_data.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JREBeTzb8ov-","outputId":"4714acfc-92dd-45fb-df1f-455c1171aeff","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3758c426-e409-44b3-9abc-18c509d32165","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+--------------------+---------+-------+--------------------+------+---------------+\n|task_id|            sentence|tkn_start|tkn_end|               chunk|entity|assertion_label|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\n|      1|The Swedish East ...|        1|      4|Swedish East Indi...|   ORG|           PAST|\n|      1|The Swedish East ...|        6|      8|Svenska Ostindisk...| ALIAS|           PAST|\n|      1|The Swedish East ...|       10|     10|                SOIC| ALIAS|           PAST|\n|      1|The Swedish East ...|       14|     14|          Gothenburg|   LOC|           PAST|\n|      1|The Swedish East ...|       15|     15|              Sweden|   LOC|           PAST|\n|      1|The Swedish East ...|       17|     17|                1731|  DATE|           PAST|\n|      1|The Swedish East ...|       25|     25|               China|   LOC|           PAST|\n|      1|The Swedish East ...|       28|     29|            Far East|   LOC|           PAST|\n|      1|The venture was i...|        9|     12|Dutch East India ...|   ORG|           PAST|\n|      1|The venture was i...|       15|     18|British East Indi...|   ORG|           PAST|\n|      1|This made Gothenb...|        2|      2|          Gothenburg|   LOC|           PAST|\n|      1|Trade with China ...|        2|      2|               China|   LOC|           PAST|\n|      1|Trade with China ...|       11|     11|              Sweden|   LOC|           PAST|\n|      1|The Chinese cultu...|       34|     34|              Sweden|   LOC|           PAST|\n|      1|The company folde...|        4|      4|                1813|  DATE|           PAST|\n|      1|nevertheless, it ...|       11|     11|          Gothenburg|   LOC|           PAST|\n|      1|Background Sweden...|       16|     17|          East India|   LOC|           PAST|\n|      1|The royal privile...|        5|      8|Swedish East Indi...|   ORG|           PAST|\n|      1|The royal privile...|        9|      9|                SOIC| ALIAS|           PAST|\n|      1|The royal privile...|       27|     28|          East India|   LOC|           PAST|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+--------------------+---------+-------+--------------------+------+---------------+\ntask_id|            sentence|tkn_start|tkn_end|               chunk|entity|assertion_label|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\n      1|The Swedish East ...|        1|      4|Swedish East Indi...|   ORG|           PAST|\n      1|The Swedish East ...|        6|      8|Svenska Ostindisk...| ALIAS|           PAST|\n      1|The Swedish East ...|       10|     10|                SOIC| ALIAS|           PAST|\n      1|The Swedish East ...|       14|     14|          Gothenburg|   LOC|           PAST|\n      1|The Swedish East ...|       15|     15|              Sweden|   LOC|           PAST|\n      1|The Swedish East ...|       17|     17|                1731|  DATE|           PAST|\n      1|The Swedish East ...|       25|     25|               China|   LOC|           PAST|\n      1|The Swedish East ...|       28|     29|            Far East|   LOC|           PAST|\n      1|The venture was i...|        9|     12|Dutch East India ...|   ORG|           PAST|\n      1|The venture was i...|       15|     18|British East Indi...|   ORG|           PAST|\n      1|This made Gothenb...|        2|      2|          Gothenburg|   LOC|           PAST|\n      1|Trade with China ...|        2|      2|               China|   LOC|           PAST|\n      1|Trade with China ...|       11|     11|              Sweden|   LOC|           PAST|\n      1|The Chinese cultu...|       34|     34|              Sweden|   LOC|           PAST|\n      1|The company folde...|        4|      4|                1813|  DATE|           PAST|\n      1|nevertheless, it ...|       11|     11|          Gothenburg|   LOC|           PAST|\n      1|Background Sweden...|       16|     17|          East India|   LOC|           PAST|\n      1|The royal privile...|        5|      8|Swedish East Indi...|   ORG|           PAST|\n      1|The royal privile...|        9|      9|                SOIC| ALIAS|           PAST|\n      1|The royal privile...|       27|     28|          East India|   LOC|           PAST|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["training_data.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ET8GD3y3-17e","outputId":"5e478418-b365-4168-8de9-aab1a885d189","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d17ed221-d84e-4abc-9b55-56558caffc04","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- task_id: long (nullable = true)\n |-- sentence: string (nullable = true)\n |-- tkn_start: long (nullable = true)\n |-- tkn_end: long (nullable = true)\n |-- chunk: string (nullable = true)\n |-- entity: string (nullable = true)\n |-- assertion_label: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- task_id: long (nullable = true)\n-- sentence: string (nullable = true)\n-- tkn_start: long (nullable = true)\n-- tkn_end: long (nullable = true)\n-- chunk: string (nullable = true)\n-- entity: string (nullable = true)\n-- assertion_label: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%time training_data.count()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R6xa4jp8Szs0","outputId":"6e8e951c-f1c7-445d-af8c-f6e76182174f","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2cf2340c-566f-43df-bb1a-949fb7c280a0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">CPU times: user 442 ms, sys: 177 ms, total: 619 ms\nWall time: 2min 14s\nOut[6]: 8050</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">CPU times: user 442 ms, sys: 177 ms, total: 619 ms\nWall time: 2min 14s\nOut[6]: 8050</div>"]}}],"execution_count":0},{"cell_type":"code","source":["(train_data, test_data) = training_data.randomSplit([0.9, 0.1], seed = 100)\nprint(\"Training Dataset Count: \" + str(training_data.count()))\nprint(\"Test Dataset Count: \" + str(test_data.count()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxcHD_Q_-_lD","outputId":"1a114990-64d0-43c1-8522-ef4a9b2145fb","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f046d60-4f81-4aab-9e46-c92320fdf6ef","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Training Dataset Count: 8050\nTest Dataset Count: 831\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Training Dataset Count: 8050\nTest Dataset Count: 831\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["train_data.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DFN_BuHU84HF","outputId":"ea931a9b-a829-4fcc-90fd-19d7aedf982d","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a85bc99-e73a-44a7-8b08-0be7aa7d829f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-------+--------------------+---------+-------+--------------------+------+---------------+\n|task_id|            sentence|tkn_start|tkn_end|               chunk|entity|assertion_label|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\n|      1|2.5 tonnes) and t...|       11|     11|          Gothenburg|   LOC|           PAST|\n|      1|2.5 tonnes) and t...|       34|     34|              Sweden|   LOC|           PAST|\n|      1|= Early attempts ...|        9|     11|  Swedish East India|   ORG|           PAST|\n|      1|= Early attempts ...|       20|     21|    Willem Usselincx|   PER|           PAST|\n|      1|= Sweden after th...|        1|      1|              Sweden|   LOC|           PAST|\n|      1|= The Royal chart...|        9|     12|Henrik König &amp; Co...|   ORG|           PAST|\n|      1|= The Royal chart...|       39|     42|   Cape of Good Hope|   LOC|           PAST|\n|      1|= The Royal chart...|       46|     46|               Japan|   LOC|           PAST|\n|      1|= The Royal chart...|       70|     70|          Gothenburg|   LOC|           PAST|\n|      1|= The Royal chart...|       79|     79|          Gothenburg|   LOC|           PAST|\n|      1|= The first octro...|        8|      8|           directors|  ROLE|           PAST|\n|      1|= The first octro...|       11|     11|                SOIC|   ORG|           PAST|\n|      1|= The first octro...|       13|     14|        Henrik König|   PER|           PAST|\n|      1|= The first octro...|       18|     19|       Frans Bedoire|   PER|           PAST|\n|      1|= The first octro...|       23|     23|           Stockholm|   LOC|           PAST|\n|      1|= The second octr...|       10|     10|                1746|  DATE|           PAST|\n|      1|= The second octr...|       11|     11|                1766|  DATE|           PAST|\n|      1|= The second octr...|       13|     13|                1786|  DATE|           PAST|\n|      1|= The second octr...|       38|     38|              Canton|   LOC|           PAST|\n|      1|A couple of weeks...|       16|     19|Anders Plomgren &amp;...|   ORG|           PAST|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+--------------------+---------+-------+--------------------+------+---------------+\ntask_id|            sentence|tkn_start|tkn_end|               chunk|entity|assertion_label|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\n      1|2.5 tonnes) and t...|       11|     11|          Gothenburg|   LOC|           PAST|\n      1|2.5 tonnes) and t...|       34|     34|              Sweden|   LOC|           PAST|\n      1|= Early attempts ...|        9|     11|  Swedish East India|   ORG|           PAST|\n      1|= Early attempts ...|       20|     21|    Willem Usselincx|   PER|           PAST|\n      1|= Sweden after th...|        1|      1|              Sweden|   LOC|           PAST|\n      1|= The Royal chart...|        9|     12|Henrik König &amp; Co...|   ORG|           PAST|\n      1|= The Royal chart...|       39|     42|   Cape of Good Hope|   LOC|           PAST|\n      1|= The Royal chart...|       46|     46|               Japan|   LOC|           PAST|\n      1|= The Royal chart...|       70|     70|          Gothenburg|   LOC|           PAST|\n      1|= The Royal chart...|       79|     79|          Gothenburg|   LOC|           PAST|\n      1|= The first octro...|        8|      8|           directors|  ROLE|           PAST|\n      1|= The first octro...|       11|     11|                SOIC|   ORG|           PAST|\n      1|= The first octro...|       13|     14|        Henrik König|   PER|           PAST|\n      1|= The first octro...|       18|     19|       Frans Bedoire|   PER|           PAST|\n      1|= The first octro...|       23|     23|           Stockholm|   LOC|           PAST|\n      1|= The second octr...|       10|     10|                1746|  DATE|           PAST|\n      1|= The second octr...|       11|     11|                1766|  DATE|           PAST|\n      1|= The second octr...|       13|     13|                1786|  DATE|           PAST|\n      1|= The second octr...|       38|     38|              Canton|   LOC|           PAST|\n      1|A couple of weeks...|       16|     19|Anders Plomgren &amp;...|   ORG|           PAST|\n+-------+--------------------+---------+-------+--------------------+------+---------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Using RoBerta Embeddings"],"metadata":{"id":"2WZDqlZA_kmb","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4ecc6258-bfd5-4a79-a8c6-6f704b7c2c3e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["roberta_embeddings = nlp.RoBertaEmbeddings.pretrained(\"roberta_embeddings_legal_roberta_base\",\"en\") \\\n    .setInputCols([\"document\", \"token\"]) \\\n    .setOutputCol(\"embeddings\") \\\n    .setMaxSentenceLength(512)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7qfJh8ap_nI2","outputId":"9bd7bd23-7a37-4961-bb94-8374b0abf7d8","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5aa50cc3-9276-49a3-8dcd-1cbc2337aa25","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">roberta_embeddings_legal_roberta_base download started this may take some time.\nApproximate size to download 447.2 MB\n\r[ | ]\r[OK!]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">roberta_embeddings_legal_roberta_base download started this may take some time.\nApproximate size to download 447.2 MB\n\r[ | ]\r[OK!]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["document = nlp.DocumentAssembler()\\\n    .setInputCol(\"sentence\")\\\n    .setOutputCol(\"document\")\n\nchunk = nlp.Doc2Chunk()\\\n    .setInputCols(\"document\")\\\n    .setOutputCol(\"doc_chunk\")\\\n    .setChunkCol(\"chunk\")\\\n    .setStartCol(\"tkn_start\")\\\n    .setStartColByTokenIndex(True)\\\n    .setFailOnMissing(False)\\\n    .setLowerCase(False)\n\ntoken = nlp.Tokenizer()\\\n    .setInputCols(['document'])\\\n    .setOutputCol('token')\n"],"metadata":{"id":"Fe0957BT_rcy","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"665f87ec-ffdb-49a0-8498-6c6ac7a53cdc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We save the test data in parquet format to use in `AssertionDLApproach()`."],"metadata":{"id":"LFTO0PlI9-3e","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"51c55a29-5454-4f7f-b947-6d7679112fa1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["assertion_pipeline = nlp.Pipeline(\n    stages = [\n    document,\n    chunk,\n    token,\n    roberta_embeddings])\n\nassertion_test_data = assertion_pipeline.fit(test_data).transform(test_data)"],"metadata":{"id":"M9u4c65G9VaC","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6516936-b09b-4d1a-8465-0a1ba8cf61f4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assertion_test_data.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r-UREmtI9Vd3","outputId":"31cd6903-4929-491c-83a7-09c5ef42d9dd","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c967fea1-3bd0-4917-a361-116cf3182b9b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[12]: [&#39;task_id&#39;,\n &#39;sentence&#39;,\n &#39;tkn_start&#39;,\n &#39;tkn_end&#39;,\n &#39;chunk&#39;,\n &#39;entity&#39;,\n &#39;assertion_label&#39;,\n &#39;document&#39;,\n &#39;doc_chunk&#39;,\n &#39;token&#39;,\n &#39;embeddings&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[12]: [&#39;task_id&#39;,\n &#39;sentence&#39;,\n &#39;tkn_start&#39;,\n &#39;tkn_end&#39;,\n &#39;chunk&#39;,\n &#39;entity&#39;,\n &#39;assertion_label&#39;,\n &#39;document&#39;,\n &#39;doc_chunk&#39;,\n &#39;token&#39;,\n &#39;embeddings&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["assertion_test_data.write.mode('overwrite').parquet('/dbfs/test_data.parquet')"],"metadata":{"id":"kBaiXx78BTLT","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"227baf61-5048-4184-9d86-46b223ff95d0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3850215879366020&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>assertion_test_data<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;overwrite&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/test_data.parquet&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, path, mode, partitionBy, compression)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1275</span>             self<span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span>partitionBy<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1276</span>         self<span class=\"ansi-blue-fg\">.</span>_set_opts<span class=\"ansi-blue-fg\">(</span>compression<span class=\"ansi-blue-fg\">=</span>compression<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1277</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1278</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1279</span>     <span class=\"ansi-green-fg\">def</span> text<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> compression<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span> lineSep<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1291.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:194)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:121)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:119)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:257)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1080)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:156)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:299)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1080)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:469)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:439)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:304)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:965)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 29 in stage 38.0 failed 4 times, most recent failure: Lost task 29.3 in stage 38.0 (TID 559) (10.139.64.6 executor 2): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.TraversableLike\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$3(HasBatchedAnnotate.scala:56)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$2(HasBatchedAnnotate.scala:55)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$1(HasBatchedAnnotate.scala:54)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:352)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:284)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:819)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1657)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:822)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:678)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2873)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2820)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2814)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2814)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1350)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1350)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1350)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3081)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3022)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3010)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1112)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2494)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2477)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:274)\n\t... 34 more\nCaused by: java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.TraversableLike\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$3(HasBatchedAnnotate.scala:56)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$2(HasBatchedAnnotate.scala:55)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$1(HasBatchedAnnotate.scala:54)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:352)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:284)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:819)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1657)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:822)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:678)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted.","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3850215879366020&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>assertion_test_data<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;overwrite&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#39;/dbfs/test_data.parquet&#39;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">parquet</span><span class=\"ansi-blue-fg\">(self, path, mode, partitionBy, compression)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1275</span>             self<span class=\"ansi-blue-fg\">.</span>partitionBy<span class=\"ansi-blue-fg\">(</span>partitionBy<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1276</span>         self<span class=\"ansi-blue-fg\">.</span>_set_opts<span class=\"ansi-blue-fg\">(</span>compression<span class=\"ansi-blue-fg\">=</span>compression<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1277</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1278</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1279</span>     <span class=\"ansi-green-fg\">def</span> text<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> path<span class=\"ansi-blue-fg\">,</span> compression<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">,</span> lineSep<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1291.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:194)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:121)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:119)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:213)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:257)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:253)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:209)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:167)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:166)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:1080)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:156)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:299)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:249)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:1080)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:469)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:439)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:304)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:965)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 29 in stage 38.0 failed 4 times, most recent failure: Lost task 29.3 in stage 38.0 (TID 559) (10.139.64.6 executor 2): java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.TraversableLike\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$3(HasBatchedAnnotate.scala:56)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$2(HasBatchedAnnotate.scala:55)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$1(HasBatchedAnnotate.scala:54)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:352)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:284)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:819)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1657)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:822)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:678)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2873)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2820)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2814)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2814)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1350)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1350)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1350)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3081)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3022)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3010)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1112)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2494)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2477)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:274)\n\t... 34 more\nCaused by: java.lang.ClassCastException: java.lang.String cannot be cast to scala.collection.TraversableLike\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$3(HasBatchedAnnotate.scala:56)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$2(HasBatchedAnnotate.scala:55)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat com.johnsnowlabs.nlp.HasBatchedAnnotate.$anonfun$batchProcess$1(HasBatchedAnnotate.scala:54)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:757)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:352)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:284)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:150)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:119)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:91)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:819)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1657)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:822)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:678)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["assertion_train_data = assertion_pipeline.fit(training_data).transform(training_data)\nassertion_train_data.write.mode('overwrite').parquet('/dbfs/train_data.parquet')"],"metadata":{"id":"tFhN6evk9ViI","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c2dff98-e4b5-4e2f-8c9e-0b382494f229","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assertion_train_data.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BtxnrvcA9VlN","outputId":"dd24e11a-9ad8-470b-9610-3d9eb1dfaac5","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"173481de-abc6-45f1-a3a0-4c7d857e6cd0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[16]: [&#39;task_id&#39;,\n &#39;sentence&#39;,\n &#39;tkn_start&#39;,\n &#39;tkn_end&#39;,\n &#39;chunk&#39;,\n &#39;entity&#39;,\n &#39;assertion_label&#39;,\n &#39;document&#39;,\n &#39;doc_chunk&#39;,\n &#39;token&#39;,\n &#39;embeddings&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[16]: [&#39;task_id&#39;,\n &#39;sentence&#39;,\n &#39;tkn_start&#39;,\n &#39;tkn_end&#39;,\n &#39;chunk&#39;,\n &#39;entity&#39;,\n &#39;assertion_label&#39;,\n &#39;document&#39;,\n &#39;doc_chunk&#39;,\n &#39;token&#39;,\n &#39;embeddings&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Graph setup"],"metadata":{"id":"uTishXbut1MS","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0c405729-885d-4d35-873f-97cfaba92446","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["We will use TFGraphBuilder annotator which can be used to create graphs in the model training pipeline. \n\nTFGraphBuilder inspects the data and creates the proper graph if a suitable version of TensorFlow (<= 2.7 ) is available. The graph is stored in the defined folder and loaded by the approach."],"metadata":{"id":"0ShZT8BBo4FY","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d42153f0-75ee-4003-a80f-200e356f3dc5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["graph_folder= \"/dbfs/tf_graphs\""],"metadata":{"id":"XhU0L1OXdaLN","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce5aef38-017b-4565-9e4b-9f88f9db9a8d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assertion_graph_builder =  legal.TFGraphBuilder()\\\n    .setModelName(\"assertion_dl\")\\\n    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n    .setLabelColumn(\"assertion_label\")\\\n    .setGraphFolder(graph_folder)\\\n    .setGraphFile(\"assertion_graph.pb\")\\\n    .setMaxSequenceLength(1200)\\\n    .setHiddenUnitsNumber(25)"],"metadata":{"id":"miNgoTjio0mL","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7044adc3-7331-4dc2-80e8-df86e887edc2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**Setting the Scope Window (Target Area) Dynamically in Assertion Status Detection Models**\n\n\nThis parameter allows you to train the Assertion Status Models to focus on specific context windows when resolving the status of a NER chunk. The window is in format `[X,Y]` being `X` the number of tokens to consider on the left of the chunk, and `Y` the max number of tokens to consider on the right. Let’s take a look at what different windows mean:\n\n\n*   By default, the window is `[-1,-1]` which means that the Assertion Status will look at all of the tokens in the sentence/document (up to a maximum of tokens set in `setMaxSentLen()` ).\n*   `[0,0]` means “don’t pay attention to any token except the ner_chunk”, what basically is not considering any context for the Assertion resolution.\n*   `[9,15]` is what empirically seems to be the best baseline, meaning that we look up to 9 tokens on the left and 15 on the right of the ner chunk to understand the context and resolve the status.\n\n\nCheck this [Scope Window Tuning Assertion Status Detection notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.1.Scope_window_tuning_assertion_status_detection.ipynb)  that illustrates the effect of the different windows and how to properly fine-tune your AssertionDLModels to get the best of them.\n\nIn our case, the best Scope Window is around [10,10]"],"metadata":{"id":"6D0Ng7nMUjJa","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"933978b0-4b46-4b36-823d-d39e6343acdf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["scope_window = [50, 50]\n\nassertionStatus = legal.AssertionDLApproach()\\\n    .setLabelCol(\"assertion_label\")\\\n    .setInputCols(\"document\", \"doc_chunk\", \"embeddings\")\\\n    .setOutputCol(\"assertion\")\\\n    .setBatchSize(128)\\\n    .setLearningRate(0.001)\\\n    .setEpochs(2)\\\n    .setStartCol(\"tkn_start\")\\\n    .setEndCol(\"tkn_end\")\\\n    .setMaxSentLen(1200)\\\n    .setEnableOutputLogs(True)\\\n    .setOutputLogsPath('dbfs:/assertion/training_logs/')\\\n    .setGraphFolder(graph_folder)\\\n    .setGraphFile(f\"{graph_folder}/assertion_graph.pb\")\\\n    .setTestDataset(path=\"dbfs:/test_data.parquet\", read_as='SPARK', options={'format': 'parquet'})\\\n    .setScopeWindow(scope_window)\n    #.setValidationSplit(0.2)\\    \n    #.setDropout(0.1)\\    "],"metadata":{"id":"BQxGbYks91go","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b53c5579-4a60-4229-ae3a-4c1e1a65e4dc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["legal_assertion_pipeline = nlp.Pipeline(\n    stages = [\n    #document,\n    #chunk,\n    #token,\n    #embeddings,\n    assertion_graph_builder,\n    assertionStatus])"],"metadata":{"id":"T2MZLeCYATrS","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7935decd-0f8b-48a0-b86a-7819a9004560","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["training_data.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yIvnuaQP91j8","outputId":"54d68047-97f2-4cc0-d8ee-b7b57948145b","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bafd96cc-5a89-465b-939b-6b645cc855f5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- task_id: long (nullable = true)\n |-- sentence: string (nullable = true)\n |-- tkn_start: long (nullable = true)\n |-- tkn_end: long (nullable = true)\n |-- chunk: string (nullable = true)\n |-- entity: string (nullable = true)\n |-- assertion_label: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- task_id: long (nullable = true)\n-- sentence: string (nullable = true)\n-- tkn_start: long (nullable = true)\n-- tkn_end: long (nullable = true)\n-- chunk: string (nullable = true)\n-- entity: string (nullable = true)\n-- assertion_label: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["assertion_train_data = spark.read.parquet('/dbfs/train_data.parquet')"],"metadata":{"id":"ueJz0aiJ_7l4","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96e88c00-5e49-4df3-8f5e-afb1505a115e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%%time\nassertion_model = legal_assertion_pipeline.fit(assertion_train_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1NCZ89T_7ol","outputId":"5a2a676d-0b61-4c59-c7db-6f2bc475c334","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d36aab39-2f76-41ed-a8a9-c47188d1ee50","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">TF Graph Builder configuration:\nModel name: assertion_dl\nGraph folder: /dbfs/tf_graphs\nGraph file name: assertion_graph.pb\nBuild params: {&#39;n_classes&#39;: 4, &#39;feat_size&#39;: 768, &#39;max_seq_len&#39;: 1200, &#39;n_hidden&#39;: 25}\nDevice mapping: no known devices.\nDevice mapping: no known devices.\nassertion_dl graph exported to /dbfs/tf_graphs/assertion_graph.pb\nCPU times: user 10.6 s, sys: 2.27 s, total: 12.9 s\nWall time: 26min 15s\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">TF Graph Builder configuration:\nModel name: assertion_dl\nGraph folder: /dbfs/tf_graphs\nGraph file name: assertion_graph.pb\nBuild params: {&#39;n_classes&#39;: 4, &#39;feat_size&#39;: 768, &#39;max_seq_len&#39;: 1200, &#39;n_hidden&#39;: 25}\nDevice mapping: no known devices.\nDevice mapping: no known devices.\nassertion_dl graph exported to /dbfs/tf_graphs/assertion_graph.pb\nCPU times: user 10.6 s, sys: 2.27 s, total: 12.9 s\nWall time: 26min 15s\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Checking the results saved in the log file"],"metadata":{"id":"30SmcTiSpnWa","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ebe215c-45ce-4fa7-9036-7c4fcf5a8651","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import os\n\nlog_files = os.listdir(\"/dbfs/assertion/training_logs/\")\nlog_files"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOiu1vuspKut","outputId":"f62ec0f2-c361-4ecb-e094-bf659e956e17","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"851e9ad0-59e3-42f6-b0e1-4b5cd6bb419f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3850215879366034&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">import</span> os\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>log_files <span class=\"ansi-blue-fg\">=</span> os<span class=\"ansi-blue-fg\">.</span>listdir<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/assertion/training_logs/&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> log_files\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;/dbfs/assertion/training_logs/&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;/dbfs/assertion/training_logs/&#39;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">FileNotFoundError</span>                         Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3850215879366034&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> <span class=\"ansi-green-fg\">import</span> os\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> \n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>log_files <span class=\"ansi-blue-fg\">=</span> os<span class=\"ansi-blue-fg\">.</span>listdir<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/assertion/training_logs/&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> log_files\n\n<span class=\"ansi-red-fg\">FileNotFoundError</span>: [Errno 2] No such file or directory: &#39;/dbfs/assertion/training_logs/&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["with open(\"/dbfs/assertion/training_logs/\"+log_files[0]) as log_file:\n    print(log_file.read())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CcQV0-fIrJHz","outputId":"4239e398-1b0e-405a-cdd3-a608ea7687ca","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ddec347e-31a8-493c-88fc-3b5f3c00002e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3850215879366035&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">with</span> open<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/assertion/training_logs/&#34;</span><span class=\"ansi-blue-fg\">+</span>log_files<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> log_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>     print<span class=\"ansi-blue-fg\">(</span>log_file<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;log_files&#39; is not defined</div>","errorSummary":"<span class=\"ansi-red-fg\">NameError</span>: name &#39;log_files&#39; is not defined","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">NameError</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3850215879366035&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span><span class=\"ansi-green-fg\">with</span> open<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;/dbfs/assertion/training_logs/&#34;</span><span class=\"ansi-blue-fg\">+</span>log_files<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> log_file<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span>     print<span class=\"ansi-blue-fg\">(</span>log_file<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">NameError</span>: name &#39;log_files&#39; is not defined</div>"]}}],"execution_count":0},{"cell_type":"code","source":["assertion_test_data = spark.read.parquet('/dbfs/test_data.parquet')"],"metadata":{"id":"bgcG00nT91nn","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ca1cdaf-4df6-4042-97f5-78c8835be9d2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["preds = assertion_model.transform(assertion_test_data).select('assertion_label','assertion.result')\n\npreds.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-k2WrFkRyQyP","outputId":"de6214ba-b53d-4eb5-a3da-53410f470169","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40aceec7-4e89-4fd6-a0b2-318dc5747053","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------+---------+\n|assertion_label|   result|\n+---------------+---------+\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|[PRESENT]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|        PRESENT|[PRESENT]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n|           PAST|   [PAST]|\n+---------------+---------+\nonly showing top 20 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+---------+\nassertion_label|   result|\n+---------------+---------+\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|[PRESENT]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n        PRESENT|[PRESENT]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n           PAST|   [PAST]|\n+---------------+---------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["preds_df = preds.toPandas()"],"metadata":{"id":"4yI73lwG2xk5","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"33977111-ab78-4829-a699-097f54480257","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["preds_df[\"result\"] = preds_df[\"result\"].apply(lambda x: x[0] if len(x) else pd.NA)\npreds_df.dropna(inplace=True)\n\npreds_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"yRXZFGlQ3Z2U","outputId":"103b1d38-b06e-4282-b878-5d1d64127b91","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2229234c-8e8a-4739-83e0-155b85c7ac4b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[29]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[29]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>assertion_label</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PAST</td>\n      <td>PRESENT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>792</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>793</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>794</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>796</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n  </tbody>\n</table>\n<p>795 rows × 2 columns</p>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>assertion_label</th>\n      <th>result</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PAST</td>\n      <td>PRESENT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>PAST</td>\n      <td>PAST</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>792</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>793</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>794</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>795</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n    <tr>\n      <th>796</th>\n      <td>POSSIBLE</td>\n      <td>POSSIBLE</td>\n    </tr>\n  </tbody>\n</table>\n<p>795 rows × 2 columns</p>\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# We are going to use sklearn to evalute the results on test dataset\nfrom sklearn.metrics import classification_report\n\nprint (classification_report( preds_df['assertion_label'], preds_df['result']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hb1kyGAE0Gn","outputId":"468f0f48-2a58-494e-c827-8df5bb141c7a","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fb67534a-f038-4b1a-bbfa-2b661abc71ce","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">              precision    recall  f1-score   support\n\n      FUTURE       0.97      0.95      0.96       123\n        PAST       0.92      0.97      0.94       278\n    POSSIBLE       0.98      0.97      0.97       177\n     PRESENT       0.95      0.89      0.92       217\n\n    accuracy                           0.95       795\n   macro avg       0.95      0.95      0.95       795\nweighted avg       0.95      0.95      0.95       795\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">              precision    recall  f1-score   support\n\n      FUTURE       0.97      0.95      0.96       123\n        PAST       0.92      0.97      0.94       278\n    POSSIBLE       0.98      0.97      0.97       177\n     PRESENT       0.95      0.89      0.92       217\n\n    accuracy                           0.95       795\n   macro avg       0.95      0.95      0.95       795\nweighted avg       0.95      0.95      0.95       795\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Saving the trained model"],"metadata":{"id":"WuJ5YZ9sXU13","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1f766042-8add-487f-afbe-fbcc5035a46c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["assertion_model.stages"],"metadata":{"id":"KBcoOwvwXV8p","colab":{"base_uri":"https://localhost:8080/"},"outputId":"85a27749-4cdc-498b-ffa1-02aa99a47912","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af8464f8-7c4d-4a82-b3ff-1b655a8331c4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[31]: [TFGraphBuilderModel_d54cd897c743, FINANCE-ASSERTION_DL_9a79432c6100]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[31]: [TFGraphBuilderModel_d54cd897c743, FINANCE-ASSERTION_DL_9a79432c6100]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Save a Spark NLP model\nassertion_model.stages[-1].write().overwrite().save('/dbfs/Assertion')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ioMW1jSrA-wg","outputId":"329f2545-0041-494f-8719-c641cb5b4c5e","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d71298a6-ef8a-4ed0-8fbc-90b25525f938","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3cee2421-dc03-4b69-9efb-56822dc8da93","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.13","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"finleg_jar","language":"python","name":"finleg_jar"},"application/vnd.databricks.v1+notebook":{"notebookName":"16.Training_Legal_Assertion","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3705761635714008},"gpuClass":"standard","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}
