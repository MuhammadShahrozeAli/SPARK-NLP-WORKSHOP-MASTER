{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77mVF2ES4S01"
      },
      "outputs": [],
      "source": [
        "! wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCiyzqtH4VCC"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "# let's start Spark with Spark NLP\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSE7xgQc4gTg",
        "outputId": "4a6296be-f211-48b9-816e-55cab2e37426"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOSCO4hg4jp9",
        "outputId": "9a4ef71b-772a-4242-b947-1b6f09468ebb"
      },
      "outputs": [],
      "source": [
        "from sparknlp.training import CoNLL\n",
        "\n",
        "training_data = CoNLL().readDataset(spark, './eng.train')\n",
        "\n",
        "training_data.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_6wrm1X4nQP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFLQsOby4rPg",
        "outputId": "10d4508e-9562-4ee0-cfa2-42ed37a3d0a9"
      },
      "outputs": [],
      "source": [
        "word2Vec = Doc2VecApproach()\\\n",
        "    .setInputCols(\"token\")\\\n",
        "    .setOutputCol(\"embeddings\")\\\n",
        "    .setMaxSentenceLength(1000)\\\n",
        "    .setStepSize(0.025)\\\n",
        "    .setMinCount(5)\\\n",
        "    .setVectorSize(100)\\\n",
        "    .setNumPartitions(1)\\\n",
        "    .setMaxIter(1)\\\n",
        "    .setSeed(42)\\\n",
        "    .setStorageRef(\"doc2vec_aclImdb\")\\\n",
        "\n",
        "nerTagger = NerDLApproach()\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    .setMaxEpochs(5)\\\n",
        "    .setLr(0.003)\\\n",
        "    .setBatchSize(8)\\\n",
        "    .setRandomSeed(0)\\\n",
        "    .setVerbose(1)\\\n",
        "    .setEvaluationLogExtended(True) \\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setIncludeConfidence(True)\\\n",
        "    .setValidationSplit(0.2)\\\n",
        "    .setOutputLogsPath('ner_logs')  # if not set, logs will be written to ~/annotator_logs\n",
        "#    .setGraphFolder('graphs') >> put your graph file (pb) under this folder if you are using a custom graph generated thru 4.1 NerDL-Graph.ipynb notebook\n",
        "#    .setEnableMemoryOptimizer() >> if you have a limited memory and a large conll file, you can set this True to train batch by batch\n",
        "\n",
        "ner_pipeline = Pipeline(stages=[\n",
        "    word2Vec,\n",
        "    nerTagger\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT4dQu328okt"
      },
      "outputs": [],
      "source": [
        "ner_model = ner_pipeline.fit(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vTRFsKV92Yz",
        "outputId": "54af004f-47dd-4038-b0b1-c1dd2c09228b"
      },
      "outputs": [],
      "source": [
        "!cd ~/annotator_logs && ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzYCO5j3EkAu",
        "outputId": "2f225170-73f1-41d9-de9a-2353e3d8610a"
      },
      "outputs": [],
      "source": [
        "!cat ~/annotator_logs/{sentimentdl.uid}.log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sparknlp.training import CoNLL\n",
        "\n",
        "test_data = CoNLL().readDataset(spark, './eng.testa')\n",
        "\n",
        "test_data.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZJuax-nFHTQ"
      },
      "outputs": [],
      "source": [
        "predictions = ner_model.transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyjerNbgFZWg",
        "outputId": "fc7b650b-5e35-4f90-f40b-2deadfd0e049"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "preds_df = predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
        "                      .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "                              F.expr(\"cols['1']\").alias(\"ground_truth\"),\n",
        "                              F.expr(\"cols['2']\").alias(\"prediction\")).toPandas()\n",
        "\n",
        "print (classification_report(preds_df['ground_truth'], preds_df['prediction']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZT6UH3NJ5heL"
      },
      "source": [
        "## Save and Restore\n",
        "### Pipeline Model\n",
        "\n",
        "It's pretty simple to save and restore an already trained Pipeline which is called `PipelineModel`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmfetBzV5nUn",
        "outputId": "181a1c0f-8ea1-4aa3-8c78-a4989ddb2920"
      },
      "outputs": [],
      "source": [
        "# this is our PipelineModel after it was trained via .fit()\n",
        "# as you can see we have all the stages inside this PipelineModel\n",
        "ner_model.stages\n",
        "# so once you save it on disk, it will include everything next time you load it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1zq3lyO8cOq"
      },
      "outputs": [],
      "source": [
        "ner_model.write().overwrite().save(\"./ner_conll03_word2vec_pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5ZAJIbx8p20",
        "outputId": "bfc48c93-9915-44ee-d63e-f9c2f65cad0f"
      },
      "outputs": [],
      "source": [
        "# let's load it back and try\n",
        "loadedPipelineModel = PipelineModel.load(\"./ner_conll03_word2vec_pipeline\")\n",
        "loadedPipelineModel.stages\n",
        "# we have all of our stages inside the loaded pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m68VFQuG9Dzf",
        "outputId": "328a4d6d-409e-4084-dee4-b685928cd9c2"
      },
      "outputs": [],
      "source": [
        "# you can use it with Spark NLP LightPipeline \n",
        "lp_loadedPipeline = LightPipeline(loadedPipelineModel)\n",
        "\n",
        "lp_loadedPipeline.annotate(\"My name is John and I am a Doctor in London!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOff6Sfr9VP6",
        "outputId": "99923eae-b173-4937-a820-3146e285bba4"
      },
      "outputs": [],
      "source": [
        "# or you can use it via DataFrame\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "dfTest = spark.createDataFrame([\n",
        "    \"This movie is a delight for those of all ages. I have seen it several times and each time I am enchanted by the characters and magic. The cast is outstanding, the special effects delightful, everything most believable.\",\n",
        "    \"This film was to put it simply rubbish. The child actors couldn't act, as can be seen by Harry's supposed surprise on learning he's a wizard. I'm a wizard! is said with such indifference you'd think he's not surprised at all.\"\n",
        "], StringType()).toDF(\"text\")\n",
        "\n",
        "loadedPipelineModel\\\n",
        "  .transform(dfTest)\\\n",
        "  .select(\"class.result\")\\\n",
        "  .show(2, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnufdTmL5oyQ"
      },
      "source": [
        "### Annotator Models\n",
        "Now let's say you would like to only save the trained annotators inside your pipeline so you can load them inside another custom Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dzzYJYQ5pJa",
        "outputId": "83da0eae-3160-4b5f-983b-3101ff277ca3"
      },
      "outputs": [],
      "source": [
        "# all we need is to access that stage and save it on disk\n",
        "ner_model.stages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0cEyPk298cd",
        "outputId": "518b3aa8-070d-4cf8-e275-11eaa246dbb2"
      },
      "outputs": [],
      "source": [
        "print(ner_model.stages[-1])\n",
        "print(ner_model.stages[-2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM16Elha-Mj3"
      },
      "outputs": [],
      "source": [
        "# let's save our ClassifierDL - let's mention it was trained by doc2vec_aclImdb as well\n",
        "ner_model.stages[-1].write().overwrite().save(\"./nerdl_conll03_word2vec_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkFvbdQA-X1T"
      },
      "outputs": [],
      "source": [
        "# and here is our trained Doc2VecModel\n",
        "ner_model.stages[-2].write().overwrite().save(\"./word2vec_conll03_model\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Train Doc2Vec and Text Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
